{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BLIP-2\n",
        "This notebook uses BLIP-2 for advanced captioning. It requires google drive access. It's a huge improvement from BLIP for photoreal images. Haven't tested on anime.\n",
        "\n",
        "Setting the torch download directory currently does nothing. It downloads locally. I think. \n",
        "\n",
        "If run on Google collab, this requires a 'premium GPU' instance type. It uses 22GB of ram at peak consumption, 38GB vram and about 40GB of hard disk space. BLIP-2 is a heavy model. You need to be using a 'premium' gpu collab instance.\n",
        "\n",
        "It requires about 0.5s/image to annotate and costs about \\$1.30/hr to run, plus about 5 minutes to load the models.There are 3300 seconds in 55 minutes, you can annotate roughly 1000 images for \\$1.30. \n",
        "\n",
        "It overwrites any existing .caption files found. \n",
        "It outputs captions to the same dir that the images exist in. \n"
      ],
      "metadata": {
        "id": "9xDQTYmnCru2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEaXmMxntqjF"
      },
      "outputs": [],
      "source": [
        "# @markdown Install Dependencies\n",
        "!pip install pillow\n",
        "!pip install transformers[Torch]\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpa4CUuUtxAc"
      },
      "outputs": [],
      "source": [
        "# @markdown Load Processor\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", offload_state_dict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncHAKwEXuLbE"
      },
      "outputs": [],
      "source": [
        "# @markdown Load Model\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16, offload_state_dict=True\n",
        ")\n",
        "model.to(device, torch.float16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3FTPI5JvWkL"
      },
      "outputs": [],
      "source": [
        "# @title Set Working Directory\n",
        "# @markdown Models take a LONG time to download, so download them to google drive\n",
        "import os\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "\n",
        "dataset_name = '' #@param {type:'string'}\n",
        "train_data_dir = f'/content/drive/MyDrive/Loras/{dataset_name}/dataset'\n",
        "\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "assert(os.path.exists(train_data_dir))\n",
        "\n",
        "blip2_model_download_dir = '/content/drive/MyDrive/torch' #@param {type:'string'}\n",
        "input_images_dir = '/content/drive/MyDrive/'\n",
        "output_captions_dir = '/content/drive/MyDrive/'\n",
        "if not os.path.exists(blip2_model_download_dir):\n",
        "  os.makedirs(blip2_model_download_dir)\n",
        "  \n",
        "# os.environ['TORCH_HOME'] = blip2_model_download_dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown generate Annotations Vectors\n",
        "min_length = 20 # @param {'type':'integer'}\n",
        "max_length = 40 # @param {'type':'integer'}\n",
        "top_p = 0.9 # @param {'type':'number'}\n",
        "\n",
        "def is_image(x):\n",
        "  ext = x.split('.')[-1]\n",
        "  if ext.lower() in ['bmp','png','jpg','jpeg']:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "from PIL import Image\n",
        "image_names = os.listdir(train_data_dir)\n",
        "image_names = [x for x in image_names if is_image(x)]\n",
        "all_inputs = []\n",
        "all_text = []\n",
        "print(image_names)\n",
        "\n",
        "for i, name in enumerate(image_names):\n",
        "  fullname = os.path.join(train_data_dir, name)\n",
        "  image = Image.open(fullname)\n",
        "    \n",
        "  inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "  generated_ids = model.generate(**inputs, min_length=20, max_length=50, top_p=0.9, repetition_penalty=2.0, num_beams=1)\n",
        "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "  base_filename = name.split('.')[0] # remove the extension\n",
        "  with open(os.path.join(train_data_dir, f'{base_filename}.caption'),'w+', encoding='utf-8') as fp:\n",
        "    fp.write(generated_text)\n",
        "    print(f'{i}:{name}:{generated_text}')"
      ],
      "metadata": {
        "id": "RpIZOaj20egU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "  \n",
        "#@markdown ### 5Ô∏è‚É£ Curate your tags\n",
        "#@markdown Modify your dataset's tags. You can run this cell multiple times with different parameters. <p>\n",
        "\n",
        "#@markdown Put an activation tag at the start of every text file. This is useful to make learning better and activate your Lora easier. Set `keep_tokens` to 1 when training.<p>\n",
        "#@markdown Common tags that are removed such as hair color, etc. will be \"absorbed\" by your activation tag.\n",
        "global_activation_tag = \"\" #@param {type:\"string\"}\n",
        "remove_tags = \"\" #@param {type:\"string\"}\n",
        "#@markdown &nbsp;\n",
        "\n",
        "#@markdown In this advanced section, you can search text files containing matching tags, and replace them with less/more/different tags. If you select the checkbox below, any extra tags will be put at the start of the file, letting you assign different activation tags to different parts of your dataset. Still, you may want a more advanced tool for this.\n",
        "search_tags = \"\" #@param {type:\"string\"}\n",
        "replace_with = \"\" #@param {type:\"string\"}\n",
        "search_mode = \"OR\" #@param [\"OR\", \"AND\"]\n",
        "new_becomes_activation_tag = False #@param {type:\"boolean\"}\n",
        "#@markdown These may be useful sometimes. Will remove existing activation tags, be careful.\n",
        "sort_alphabetically = False #@param {type:\"boolean\"}\n",
        "remove_duplicates = False #@param {type:\"boolean\"}\n",
        "\n",
        "def split_tags(tagstr):\n",
        "  return [s.strip() for s in tagstr.split(\",\") if s.strip()]\n",
        "\n",
        "activation_tag_list = split_tags(global_activation_tag)\n",
        "remove_tags_list = split_tags(remove_tags)\n",
        "search_tags_list = split_tags(search_tags)\n",
        "replace_with_list = split_tags(replace_with)\n",
        "replace_new_list = [t for t in replace_with_list if t not in search_tags_list]\n",
        "\n",
        "replace_with_list = [t for t in replace_with_list if t not in replace_new_list]\n",
        "replace_new_list.reverse()\n",
        "activation_tag_list.reverse()\n",
        "\n",
        "remove_count = 0\n",
        "replace_count = 0\n",
        "\n",
        "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "    tags = [s.strip() for s in f.read().split(\",\")]\n",
        "\n",
        "  if remove_duplicates:\n",
        "    tags = list(set(tags))\n",
        "  if sort_alphabetically:\n",
        "    tags.sort()\n",
        "\n",
        "  for rem in remove_tags_list:\n",
        "    if rem in tags:\n",
        "      remove_count += 1\n",
        "      tags.remove(rem)\n",
        "\n",
        "  if \"AND\" in search_mode and all(r in tags for r in search_tags_list) \\\n",
        "      or \"OR\" in search_mode and any(r in tags for r in search_tags_list):\n",
        "    replace_count += 1\n",
        "    for rem in search_tags_list:\n",
        "      if rem in tags:\n",
        "        tags.remove(rem)\n",
        "    for add in replace_with_list:\n",
        "      if add not in tags:\n",
        "        tags.append(add)\n",
        "    for new in replace_new_list:\n",
        "      if new_becomes_activation_tag:\n",
        "        if new in tags:\n",
        "          tags.remove(new)\n",
        "        tags.insert(0, new)\n",
        "      else:\n",
        "        if new not in tags:\n",
        "          tags.append(new)\n",
        "\n",
        "  for act in activation_tag_list:\n",
        "    if act in tags:\n",
        "      tags.remove(act)\n",
        "    tags.insert(0, act)\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "    f.write(\", \".join(tags))\n",
        "\n",
        "if remove_tags:\n",
        "  print(f\"\\nüöÆ Removed {remove_count} tags.\")\n",
        "if search_tags:\n",
        "  print(f\"\\nüí´ Replaced in {replace_count} files.\")\n",
        "print(\"\\n‚úÖ Done!\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sF9MMr7LX7w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEHNh0P8Yyxi"
      },
      "outputs": [],
      "source": [
        "#@title ## 4.3. Merge Annotation Into JSON \n",
        "import os\n",
        "%store -r\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "\n",
        "#@markdown Cleaning tags and captions, then merges them into a single JSON file, which will be used as the input for the bucketing section.\n",
        "meta_clean = f\"/content/drive/MyDrive/Loras/{dataset_name}/meta_clean.json\"\n",
        "parent_folder = os.path.dirname(meta_clean)\n",
        "meta_cap_dd = f\"{parent_folder}/meta_cap_dd.json\"\n",
        "meta_cap = f\"{parent_folder}/meta_cap.json\"\n",
        "\n",
        "os.makedirs(parent_folder, exist_ok=True)\n",
        "\n",
        "if os.path.isdir(train_data_dir):\n",
        "  if any(file.endswith('.caption') for file in os.listdir(train_data_dir)):\n",
        "    !python merge_captions_to_metadata.py \\\n",
        "      {train_data_dir} \\\n",
        "      {meta_cap}\n",
        "\n",
        "  if any(file.endswith('.txt') for file in os.listdir(train_data_dir)):\n",
        "    !python merge_dd_tags_to_metadata.py \\\n",
        "      {train_data_dir} \\\n",
        "      {meta_cap_dd}\n",
        "else:\n",
        "  print(\"train_data_dir does not exist or is not a directory.\")\n",
        "\n",
        "if os.path.exists(meta_cap):\n",
        "  !python merge_dd_tags_to_metadata.py \\\n",
        "    {train_data_dir} \\\n",
        "    --in_json {meta_cap} \\\n",
        "    {meta_cap_dd}\n",
        "\n",
        "if os.path.exists(meta_cap_dd):\n",
        "  !python clean_captions_and_tags.py \\\n",
        "    {meta_cap_dd} \\\n",
        "    {meta_clean}\n",
        "elif os.path.exists(meta_cap):\n",
        "  !python clean_captions_and_tags.py \\\n",
        "    {meta_cap} \\\n",
        "    {meta_clean}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0FeUeRKDVb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}